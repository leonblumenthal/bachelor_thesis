{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on A9 Dataset\n",
    "\n",
    "> The basis for these evaluations are the saved detections for each model. In productive use, the model output is directly used and some calculations are performed on the GPU, however the algorithm is equal. The results might only differ due to rounding errors and alike.\n",
    "\n",
    "Frames are split into training and test set, such that they have a similar distribution of number of labels and camers. The training set (~1/4 of frames) frames is used to calculate the optimal hyperparameters. The ratio does not need to be high and therefore more frames are included in the test set which improves the evaluation. With the optimized hyperparameters, the approach will be evaluated on the test set. The hyperparameters are optimized for each instance segmentation model individually and the results on the test can can be compared. \n",
    "\n",
    "## Metrics\n",
    "\n",
    "At first, corresponding predictions and labels have to be matched (to compare location and dimensions later on). This works as follows:\n",
    "1. Calculate pairwise distances between labels and predictions on image\n",
    "2. Match pairs if they are both their nearest neighbor\n",
    "3. Remove unmatched labels, unmatched predictions, and matches with labels further away than the distance cutoff\n",
    "\n",
    "The cutoff is needed because the frames contain vehicles too far away for labeling and detecting. Therefore, it is also dependent on the perspective. The pairs are matched first to not count e.g. a label, slightly within the cutoff, as unmatched if the corresponding prediction is slightly outside. In productive use, the predictions to far awaycan just be ignored.\n",
    "\n",
    "After matching, precision, recall and F1 score can be calculated (micro average) with matches as true positives (*TP*), unmachted labels as false negatives (*FN*), and unmatched predictions as false positives (*FP*).\n",
    "\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "$$f1 = 2* \\frac{precision * recall}{precision + recall}$$\n",
    "\n",
    "The matched pairs can be used to calculate the mean average error (*MAE*) for location and dimensions. The differences in predicted and labeled categories can be evaluated using a confusion matrix.\n",
    "\n",
    "$$ MAE = \\frac{1}{n} \\sum_{i}^n |\\hat{y}_i - y_i| $$\n",
    "\n",
    "## Optimal Hyperparameters\n",
    "\n",
    "The hyperparameters are:\n",
    "1. score threshold (for scores of results of instance segmentation)\n",
    "2. mask width threshold (for approximated widths of instance masks)\n",
    "3. image margin threshold (for minimum distances of instance masks to image edge)\n",
    "4. vehicle dimensions (min, max, and mean of length, width, and height for each vehicle category)\n",
    "5. vertical contour shift (number of pixels contours are shifted down on image)\n",
    "\n",
    "\n",
    "First, the vehicle dimensions can be calculated, including the alignment of heights for better distinction between cars, vans, and trucks. Then, thresholds 1 and 2 are evaluated on the f1 score. The image margin threshold can be set to zero since it only exists to not make predictions on cutoff vehicles. At last, the vertical contour shift can be optimized based on location- and dimenisions errors.\n",
    "\n",
    "## Test Set Evaluation\n",
    "\n",
    "For every instance segmentation model, with the individually optimal hyperparameters, the test set can be formally evaluated on precision, recall, f1 score, location error, dimensions error, and category error. These values are split per label category, perspective, and distance to camera.\n",
    "\n",
    "## Qualitative Results\n",
    "\n",
    "Show predictions and labels on frames and 2D views inclusing common false positives and false negatives.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows import from parent directory.\n",
    "import sys; sys.path.append('..')\n",
    "\n",
    "\n",
    "import os\n",
    "from typing import Dict, Iterator, List, Set, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import src.visualization_utils as viz\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from plotly.subplots import make_subplots\n",
    "from src.approaches.fixed_angle import create_predictions\n",
    "from src.loaders import DetectionsLoader, FrameLoader, LabelsLoader\n",
    "from src.models import DirectionLine, Vehicle\n",
    "from src.perspective import Perspective\n",
    "from src.providentia_utils import (\n",
    "    extract_camera_name,\n",
    "    match_perspective,\n",
    "    parse_perspectives,\n",
    ")\n",
    "\n",
    "pio.templates['tight'] = dict(\n",
    "    layout=dict(\n",
    "        margin=dict(l=10, r=10, t=10, b=10),\n",
    "        font=dict(family='charter', color='black', size=14),\n",
    "    )\n",
    ")\n",
    "pio.templates.default = 'plotly_white+tight'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels, frames, and perspectives for A9 dataset\n",
    "labels_loader = LabelsLoader('../data/r0_a9_dataset/labels')\n",
    "frame_loader = FrameLoader('../data/r0_a9_dataset/images')\n",
    "perspectives = parse_perspectives('../data/profusion_r0_dataset/r0_s1/05_calibration')\n",
    "\n",
    "# Detections lodader for every model\n",
    "detections_dir_path = '../data/detections/r0_a9_dataset'\n",
    "detections_loaders = {\n",
    "    name: DetectionsLoader(os.path.join(detections_dir_path, name))\n",
    "    for name in sorted(os.listdir(detections_dir_path))\n",
    "}\n",
    "\n",
    "# Different cutoff based on perspective\n",
    "cutoffs = {\n",
    "    perspective: 250 if '50mm' in camera else 125\n",
    "    for camera, perspective in perspectives.items()\n",
    "}\n",
    "\n",
    "# Fixed parameters\n",
    "direction_line = DirectionLine(np.array([[0, 1]]).T, 0)\n",
    "label_mappings = {\n",
    "    'coco': {2: 'CAR', 5: 'BUS', 7: 'TRUCK'},\n",
    "}\n",
    "\n",
    "output_dir = 'output/fixed_angle'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split frames into training and test with 1/4 ratio\n",
    "split_ratio = 1 / 4\n",
    "\n",
    "# Create dataframe for camera and number of labels per frame.\n",
    "data = {\n",
    "    'camera': [],\n",
    "    'labels': [],\n",
    "}\n",
    "for labels, labels_data in labels_loader.load_items():\n",
    "    camera = extract_camera_name(labels_data)\n",
    "    data['camera'].append(camera)\n",
    "    data['labels'].append(len(labels))\n",
    "frames_df = pd.DataFrame(data)\n",
    "\n",
    "# Create split.\n",
    "split = np.zeros(len(frames_df), dtype=bool)\n",
    "for camera, part in frames_df.camera.value_counts().items():\n",
    "    num_frames_to_sample = round(part * split_ratio)\n",
    "    selected_camera_frames = np.random.choice(\n",
    "        frames_df[frames_df.camera == camera].index,\n",
    "        size=num_frames_to_sample,\n",
    "        replace=False,\n",
    "    )\n",
    "    split[selected_camera_frames] = True\n",
    "frames_df['split'] = split\n",
    "\n",
    "training_indices = np.argwhere(split).squeeze()\n",
    "test_indices = np.argwhere(~split).squeeze()\n",
    "\n",
    "# Compare distributions on frames per camera and mean number of labels per camera.\n",
    "df = frames_df.groupby(['camera', 'split']).count().reset_index()\n",
    "df.loc[df.split, 'labels'] /= df[df.split].labels.sum()\n",
    "df.loc[~df.split, 'labels'] /= df[~df.split].labels.sum()\n",
    "px.bar(\n",
    "    df,\n",
    "    x='camera',\n",
    "    y='labels',\n",
    "    color='split',\n",
    "    barmode='group',\n",
    "    labels={'labels': 'percentage of labels'},\n",
    "    height=200,\n",
    ").show()\n",
    "px.bar(\n",
    "    frames_df.groupby(['camera', 'split']).mean().reset_index(),\n",
    "    x='camera',\n",
    "    y='labels',\n",
    "    color='split',\n",
    "    barmode='group',\n",
    "    labels={'labels': 'mean number of labels'},\n",
    "    height=200,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_dimension_values_mapping(\n",
    "    labels_loader: LabelsLoader,\n",
    "    frame_indices: Iterator[int],\n",
    "    category_merges: Dict[str, str] = {'TRAILER': 'TRUCK'},\n",
    "    wanted_categories: Set[str] = {'CAR', 'VAN', 'BUS', 'TRUCK'},\n",
    "    height_partitions: List[List[str]] = [['CAR'], ['VAN'], ['BUS', 'TRUCK']],\n",
    ") -> Dict[str, Tuple]:\n",
    "    \"\"\"\n",
    "    Produce min-, mean-, and max values for each dimension per wanted category.\n",
    "    Also partition height values based on averaged mean values between neigbors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Aggregate data for all labels.\n",
    "    data = {'category': [], 'length': [], 'width': [], 'height': []}\n",
    "    for labels, _ in labels_loader.load_items(frame_indices):\n",
    "        for label in labels:\n",
    "            # Combine trailers and trucks.\n",
    "            if label.category in category_merges:\n",
    "                label.category = category_merges[label.category]\n",
    "\n",
    "            if label.category not in wanted_categories:\n",
    "                continue\n",
    "\n",
    "            length, width, height = label.dimensions\n",
    "            data['length'].append(length)\n",
    "            data['width'].append(width)\n",
    "            data['height'].append(height)\n",
    "            data['category'].append(label.category)\n",
    "\n",
    "    labels_df = pd.DataFrame(data)\n",
    "\n",
    "    # Create min-, mean-, and max values for each dimension per category.\n",
    "    category_group = labels_df[['length', 'width', 'height']].groupby(\n",
    "        labels_df.category\n",
    "    )\n",
    "    quantiles = category_group.quantile([0.02, 0.98])\n",
    "    means = category_group.mean()\n",
    "\n",
    "    mapping = {}\n",
    "    for category in labels_df.category.unique():\n",
    "        lower, higher = quantiles.loc[category].values\n",
    "        mean = means.loc[category].values\n",
    "        mapping[category] = np.array([lower, mean, higher]).T.tolist()\n",
    "\n",
    "    # Partition heights based on average of neighbor means.\n",
    "    height_means = [\n",
    "        sum(mapping[c][2][1] for c in cats) / len(cats) for cats in height_partitions\n",
    "    ]\n",
    "    boundaries = [(a + b) / 2 for a, b in zip(height_means, height_means[1:])]\n",
    "\n",
    "    for max_cats, min_cats, boundary in zip(\n",
    "        height_partitions, height_partitions[1:], boundaries\n",
    "    ):\n",
    "        for c in max_cats:\n",
    "            mapping[c][2][2] = boundary\n",
    "        for c in min_cats:\n",
    "            mapping[c][2][0] = boundary\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_values_mapping = produce_dimension_values_mapping(labels_loader, training_indices)\n",
    "dimension_values_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_centers(\n",
    "    vehicles: List[Vehicle], perspective: Perspective\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Project vehicle centers (ground location + height/2) into image plane.\"\"\"\n",
    "\n",
    "    centers = np.hstack(\n",
    "        [\n",
    "            vehicle.location + np.array([[0, 0, vehicle.dimensions[2] / 2]]).T\n",
    "            for vehicle in vehicles\n",
    "        ]\n",
    "    )\n",
    "    centers = perspective.project_to_image(centers)\n",
    "\n",
    "    return centers\n",
    "\n",
    "\n",
    "def calculate_distance_to_camera(vehicle: Vehicle, perspective: Perspective) -> float:\n",
    "    return np.abs(vehicle.location[0, 0] - perspective.translation[0, 0])\n",
    "\n",
    "\n",
    "def match(\n",
    "    predictions: List[Vehicle],\n",
    "    labels: List[Vehicle],\n",
    "    perspective: Perspective,\n",
    "    distance_cutoff: float = 250,\n",
    ") -> Tuple[List[Tuple[Vehicle, Vehicle]], List[Vehicle], List[Vehicle]]:\n",
    "    \"\"\"\n",
    "    Match predictions with labels and remove labels further away than distance cutoff.\n",
    "    Return matches, unmatched predictions, unmatched labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not predictions or not labels:\n",
    "        return [], predictions, labels\n",
    "\n",
    "    # Calculate pairwise distances between label- and prediction centers on image.\n",
    "    label_centers = calculate_image_centers(labels, perspective)\n",
    "    prediction_centers = calculate_image_centers(predictions, perspective)\n",
    "    distances = np.linalg.norm(\n",
    "        label_centers[..., None] - prediction_centers[:, None, :], ord=1, axis=0\n",
    "    )\n",
    "\n",
    "    # Calculate nearest label for each prediction and vice-versa.\n",
    "    pred_nearest = distances.argmin(0)\n",
    "    lab_nearest = distances.argmin(1)\n",
    "\n",
    "    # Match predictions and labels if they are both nearest to each other.\n",
    "    matches_i = []\n",
    "    unmatched_predictions_i = set(range(len(predictions)))\n",
    "    unmatched_labels_i = set(range(len(labels)))\n",
    "    for pred, lab in enumerate(pred_nearest):\n",
    "        if pred == lab_nearest[lab]:\n",
    "            matches_i.append((pred, lab))\n",
    "            unmatched_predictions_i.remove(pred)\n",
    "            unmatched_labels_i.remove(lab)\n",
    "\n",
    "    # Remove all labels (including matched ones)\n",
    "    # and unmatched predictions further away than distance cutoff.\n",
    "    # The matching is done before that to allow matching at the cutoff.\n",
    "\n",
    "    matches = []\n",
    "    for pi, li in matches_i:\n",
    "        label = labels[li]\n",
    "        if calculate_distance_to_camera(label, perspective) > distance_cutoff:\n",
    "            continue\n",
    "\n",
    "        prediction = predictions[pi]\n",
    "        matches.append((prediction, label))\n",
    "\n",
    "    unmatched_predictions = []\n",
    "    for i in unmatched_predictions_i:\n",
    "        prediction = predictions[i]\n",
    "        if calculate_distance_to_camera(prediction, perspective) > distance_cutoff:\n",
    "            continue\n",
    "\n",
    "        unmatched_predictions.append(prediction)\n",
    "\n",
    "    unmatched_labels = []\n",
    "    for i in unmatched_labels_i:\n",
    "        label = labels[i]\n",
    "        if calculate_distance_to_camera(label, perspective) > distance_cutoff:\n",
    "            continue\n",
    "\n",
    "        unmatched_labels.append(label)\n",
    "\n",
    "    return matches, unmatched_predictions, unmatched_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresholds(\n",
    "    model_name: str, score_values: Iterator[float], mask_width_values: Iterator[float]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Find best threshold values for score and mask width based on f1 score.\"\"\"\n",
    "\n",
    "    max_f1 = 0\n",
    "    max_thresholds = None\n",
    "\n",
    "    for score in score_values:\n",
    "        for mask_width in mask_width_values:\n",
    "\n",
    "            thresholds = {'score': score, 'mask_width': mask_width, 'edge_margin': 0}\n",
    "\n",
    "            num_matches = 0\n",
    "            num_unmatched_predictions = 0\n",
    "            num_unmatched_labels = 0\n",
    "\n",
    "            detections_loader = detections_loaders[model_name]\n",
    "            for detections, (labels, labels_data) in zip(\n",
    "                detections_loader.load_items(training_indices),\n",
    "                labels_loader.load_items(training_indices),\n",
    "            ):\n",
    "                perspective = match_perspective(perspectives, labels_data)\n",
    "\n",
    "                predictions = create_predictions(\n",
    "                    detections,\n",
    "                    perspective,\n",
    "                    direction_line,\n",
    "                    thresholds,\n",
    "                    0,\n",
    "                    label_mappings['coco'],\n",
    "                    dimension_values_mapping,\n",
    "                )\n",
    "\n",
    "                matches, unmatched_predictions, unmatched_labels = match(\n",
    "                    predictions, labels, perspective, cutoffs[perspective]\n",
    "                )\n",
    "\n",
    "                num_matches += len(matches)\n",
    "                num_unmatched_predictions += len(unmatched_predictions)\n",
    "                num_unmatched_labels += len(unmatched_labels)\n",
    "\n",
    "            precision = num_matches / (num_matches + num_unmatched_predictions + 1e-8)\n",
    "            recall = num_matches / (num_matches + num_unmatched_labels + 1e-8)\n",
    "            f1_score = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "            if f1_score > max_f1:\n",
    "                max_f1 = f1_score\n",
    "                max_thresholds = thresholds\n",
    "\n",
    "    return max_thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_vertical_contour_shift(\n",
    "    model_name: str,\n",
    "    thresholds: Dict[str, float],\n",
    "    values: Iterator[int],\n",
    ") -> float:\n",
    "    \"\"\"Find best vertical contour shift based on combined x location and length dimension MAEs.\"\"\"\n",
    "\n",
    "    best_vertical_contour_shift = None\n",
    "    best_error = 1e8\n",
    "\n",
    "    for vertical_contour_shift in values:\n",
    "        location_errors = []\n",
    "        dimensions_errors = []\n",
    "\n",
    "        detections_loader = detections_loaders[model_name]\n",
    "        for detections, (labels, labels_data) in zip(\n",
    "            detections_loader.load_items(training_indices),\n",
    "            labels_loader.load_items(training_indices),\n",
    "        ):\n",
    "            perspective = match_perspective(perspectives, labels_data)\n",
    "\n",
    "            predictions = create_predictions(\n",
    "                detections,\n",
    "                perspective,\n",
    "                direction_line,\n",
    "                thresholds,\n",
    "                vertical_contour_shift,\n",
    "                label_mappings['coco'],\n",
    "                dimension_values_mapping,\n",
    "            )\n",
    "\n",
    "            matches, _, _ = match(\n",
    "                predictions, labels, perspective, cutoffs[perspective]\n",
    "            )\n",
    "\n",
    "            for prediction, label in matches:\n",
    "                location_errors.append(\n",
    "                    np.linalg.norm(prediction.location - label.location, 1)\n",
    "                )\n",
    "                dimensions_errors.append(\n",
    "                    np.linalg.norm(\n",
    "                        np.subtract(prediction.dimensions, label.dimensions), 1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        error = np.mean(location_errors) + np.mean(dimensions_errors)\n",
    "\n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_vertical_contour_shift = vertical_contour_shift\n",
    "\n",
    "    return best_vertical_contour_shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best hyperparemeters for each model.\n",
    "\n",
    "score_threshold_values = np.linspace(0.49, 0.61, 13)\n",
    "mask_width_threshold_values = [0, 5, 10, 15, 20, 25, 30]\n",
    "vertical_contour_shift_values = np.arange(-1, 4)\n",
    "\n",
    "best_parameters = {}\n",
    "\n",
    "for model_name in detections_loaders.keys():\n",
    "    if 'cityscapes' in model_name:\n",
    "        continue\n",
    "\n",
    "    thresholds = find_best_thresholds(model_name, score_threshold_values, mask_width_threshold_values)\n",
    "    vertical_contour_shift = find_best_vertical_contour_shift(model_name, thresholds, vertical_contour_shift_values)\n",
    "\n",
    "    best_parameters[model_name] = (thresholds, vertical_contour_shift)\n",
    "\n",
    "    print(f'{model_name}:')\n",
    "    print(f'  {thresholds=}')\n",
    "    print(f'  {vertical_contour_shift=}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model on the test set.\n",
    "\n",
    "matches_dfs = {}\n",
    "\n",
    "for model_name, (thresholds, vertical_contour_shift) in best_parameters.items():\n",
    "\n",
    "    num_matches = 0\n",
    "    num_unmatched_predictions = 0\n",
    "    num_unmatched_labels = 0\n",
    "    location_errors = []\n",
    "    dimensions_errors = []\n",
    "    prediction_categories = []\n",
    "    label_categories = []\n",
    "\n",
    "    matches_data = {\n",
    "        'prediction_category': [],\n",
    "        'label_category': [],\n",
    "        'x_error': [],\n",
    "        'y_error': [],\n",
    "        'z_error': [],\n",
    "        'length_error': [],\n",
    "        'width_error': [],\n",
    "        'height_error': [],\n",
    "        'camera': [],\n",
    "        'camera_distance': [],\n",
    "    }\n",
    "\n",
    "    detections_loader = detections_loaders[model_name]\n",
    "    for detections, (labels, labels_data) in zip(\n",
    "        detections_loader.load_items(test_indices),\n",
    "        labels_loader.load_items(test_indices),\n",
    "    ):\n",
    "        perspective = match_perspective(perspectives, labels_data)\n",
    "        camera = extract_camera_name(labels_data)\n",
    "        camera = camera.replace('_camera_basler', '').replace('_', '-')\n",
    "\n",
    "        predictions = create_predictions(\n",
    "            detections,\n",
    "            perspective,\n",
    "            direction_line,\n",
    "            {**thresholds, 'edge_margin': 10},\n",
    "            vertical_contour_shift,\n",
    "            label_mappings['coco'],\n",
    "            dimension_values_mapping,\n",
    "        )\n",
    "\n",
    "        predictions2 = create_predictions(\n",
    "            detections,\n",
    "            perspective,\n",
    "            direction_line,\n",
    "            thresholds,\n",
    "            vertical_contour_shift,\n",
    "            label_mappings['coco'],\n",
    "            dimension_values_mapping,\n",
    "        )\n",
    "\n",
    "        matches, unmatched_predictions, unmatched_labels = match(\n",
    "            predictions, labels, perspective, cutoffs[perspective]\n",
    "        )\n",
    "\n",
    "        matches2, unmatched_predictions2, unmatched_labels2 = match(\n",
    "            predictions2, labels, perspective, cutoffs[perspective]\n",
    "        )\n",
    "\n",
    "        for prediction, label in matches:\n",
    "            x, y, z = (prediction.location - label.location).squeeze()\n",
    "            matches_data['x_error'].append(x)\n",
    "            matches_data['y_error'].append(y)\n",
    "            matches_data['z_error'].append(z)\n",
    "\n",
    "            length, width, height = np.subtract(prediction.dimensions, label.dimensions)\n",
    "            matches_data['length_error'].append(length)\n",
    "            matches_data['width_error'].append(width)\n",
    "            matches_data['height_error'].append(height)\n",
    "\n",
    "            matches_data['prediction_category'].append(prediction.category)\n",
    "            matches_data['label_category'].append(label.category)\n",
    "\n",
    "            matches_data['camera'].append(camera)\n",
    "            matches_data['camera_distance'].append(\n",
    "                calculate_distance_to_camera(label, perspective)\n",
    "            )\n",
    "\n",
    "        num_matches += len(matches2)\n",
    "        num_unmatched_predictions += len(unmatched_predictions2)\n",
    "        num_unmatched_labels += len(unmatched_labels2)\n",
    "\n",
    "    # Calculate precision, recall and f1 score with micro averaging.\n",
    "    precision = num_matches / (num_matches + num_unmatched_predictions + 1e-8)\n",
    "    recall = num_matches / (num_matches + num_unmatched_labels + 1e-8)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    matches_df = pd.DataFrame(matches_data)\n",
    "\n",
    "    matches_dfs[model_name] = matches_df\n",
    "\n",
    "    location_mae = (\n",
    "        matches_df[['x_error', 'y_error', 'z_error']].abs().mean().round(2).to_list()\n",
    "    )\n",
    "    dimensions_mae = (\n",
    "        matches_df[['length_error', 'width_error', 'height_error']]\n",
    "        .abs()\n",
    "        .mean()\n",
    "        .round(2)\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    print(model_name)\n",
    "    print(f'{f1_score=:.4f}, {precision=:.4f}, {recall=:.4f}')\n",
    "    print(f'{location_mae=}, {dimensions_mae=}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for category from labels to predictions\n",
    "\n",
    "model_name = 'coco_yolact_resnet101_edge'\n",
    "\n",
    "matches_df = matches_dfs[model_name]\n",
    "\n",
    "confusion_df = pd.crosstab(\n",
    "    matches_df.prediction_category,\n",
    "    np.array([c.split('_')[0] for c in matches_df.label_category]),\n",
    "    normalize='columns',\n",
    "    rownames=['predictions'],\n",
    "    colnames=['labels'],\n",
    ")\n",
    "fig = px.imshow(confusion_df.round(2), text_auto=True, color_continuous_scale='blues')\n",
    "fig.update_layout(coloraxis_showscale=False, xaxis_side='top', font_size=18, margin_b=0)\n",
    "fig.write_image(os.path.join(output_dir, 'confusion.pdf'))\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE values per label category\n",
    "\n",
    "error_columns = [\n",
    "    'x_error',\n",
    "    'y_error',\n",
    "    'z_error',\n",
    "    'length_error',\n",
    "    'width_error',\n",
    "    'height_error',\n",
    "]\n",
    "\n",
    "matches_df[error_columns].abs().groupby(matches_df.label_category).mean().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE values per camera\n",
    "\n",
    "matches_df[error_columns].abs().groupby(matches_df.camera).mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE values per camera and distance\n",
    "\n",
    "distance_intervals = 25 * (matches_df.camera_distance // 25)\n",
    "distance_df = (\n",
    "    matches_df[['x_error', 'y_error', 'length_error', 'width_error', 'height_error']]\n",
    "    .abs()\n",
    "    .groupby([matches_df.camera, distance_intervals])\n",
    "    .mean()\n",
    "    .round(2)\n",
    ")\n",
    "distance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure for MAE values per camera and distance\n",
    "\n",
    "fig = make_subplots(5, 1, shared_xaxes=True, vertical_spacing=0.02)\n",
    "fig.update_layout(\n",
    "    width=1200,\n",
    "    height=1600,\n",
    "    font_size=24,\n",
    "    legend=dict(\n",
    "        x=0.04,\n",
    "        y=1.04,\n",
    "        orientation='h',\n",
    "        font=dict(size=28),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "df = distance_df.reset_index()\n",
    "df.camera_distance += 12.5\n",
    "\n",
    "\n",
    "for i, camera in enumerate(df.camera.unique()):\n",
    "    for j, (name, values) in enumerate(distance_df.loc[camera].iteritems()):\n",
    "        fig.add_scatter(\n",
    "            y=values,\n",
    "            x=values.index + 12.5,\n",
    "            row=j + 1,\n",
    "            col=1,\n",
    "            showlegend=j == 0,\n",
    "            name=camera,\n",
    "            line_color=px.colors.qualitative.Plotly[i],\n",
    "            line_width=6,\n",
    "            marker_size=10,\n",
    "        )\n",
    "\n",
    "y_titles = [\n",
    "    'x location error [m]',\n",
    "    'y location error [m]',\n",
    "    'length error [m]',\n",
    "    'width error [m]',\n",
    "    'height error [m]',\n",
    "]\n",
    "\n",
    "for i in range(1, 6):\n",
    "    fig.update_layout(\n",
    "        **{\n",
    "            f'xaxis{i}': dict(\n",
    "                zerolinecolor='black',\n",
    "                linecolor='black',\n",
    "                linewidth=3,\n",
    "                zerolinewidth=3,\n",
    "                gridcolor='grey',\n",
    "            ),\n",
    "            f'yaxis{i}': dict(gridcolor='black', title=y_titles[i - 1]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "fig.update_layout(xaxis5=dict(title='camera distance[m]'))\n",
    "\n",
    "\n",
    "fig.write_image(os.path.join(output_dir, 'errors_distance.pdf'))\n",
    "\n",
    "fig.show('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative results\n",
    "\n",
    "# s40 50mm, s40 50mm, s50 16mm, s40 16mm\n",
    "indices = [640, 364, 460, 416]\n",
    "\n",
    "color_mapping = {\n",
    "    category: color\n",
    "    for category, color in zip(\n",
    "        dimension_values_mapping.keys(), px.colors.qualitative.Plotly\n",
    "    )\n",
    "}\n",
    "\n",
    "frames = {}\n",
    "\n",
    "thresholds, vertical_contour_shift = best_parameters[model_name]\n",
    "for frame, detections, (_, labels_data) in zip(\n",
    "    frame_loader.load_items(indices),\n",
    "    detections_loaders[model_name].load_items(indices),\n",
    "    labels_loader.load_items(indices),\n",
    "):\n",
    "    perspective = match_perspective(perspectives, labels_data)\n",
    "    camera = extract_camera_name(labels_data)\n",
    "\n",
    "    predictions = create_predictions(\n",
    "        detections,\n",
    "        perspective,\n",
    "        direction_line,\n",
    "        {**thresholds, 'mask_width': 50},\n",
    "        vertical_contour_shift,\n",
    "        label_mappings['coco'],\n",
    "        dimension_values_mapping,\n",
    "    )\n",
    "\n",
    "    frames[camera] = viz.overlay_3d_vehicles(\n",
    "        frame,\n",
    "        predictions,\n",
    "        perspective,\n",
    "        box_colors=[color_mapping[vehicle.category] for vehicle in predictions],\n",
    "        box_thickness=4,\n",
    "    )\n",
    "\n",
    "fig = plt.figure(figsize=(16, 3), dpi=400)\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(1, 4), axes_pad=0.2)\n",
    "for ax, (camera, img) in zip(grid, frames.items()):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(camera, {'fontsize': 8})\n",
    "plt.show()\n",
    "\n",
    "for camera, img in frames.items():\n",
    "    plt.imsave(os.path.join(output_dir, camera + '.png'), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa9c5a8f1f4deddb2ec8235298079442a1fa1dd657a205c118aef888c2110fcf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('3.9.0': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
